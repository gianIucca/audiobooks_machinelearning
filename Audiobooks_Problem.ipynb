{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks business case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "You are given data from an Audiobook App. Logically, it relates to the audio versions of books only. Each customer in the database has made a purchase at least once, that's why he/she is in the database. The purpose of this project is to create a machine learning algorithm based on our available data that can predict if a customer will buy again from the Audiobook company.\n",
    "\n",
    "The main idea is that if a customer has a low probability of coming back, there is no reason to spend any money on advertising to him/her. If we can focus our efforts solely on customers that are likely to convert again, we can make great savings. Moreover, this model can identify the most important metrics for a customer to come back again. Identifying new customers creates value and growth opportunities.\n",
    "\n",
    "You have a .csv summarizing the data. There are several variables: Customer ID, Book length overall (sum of the minute length of all purchases), Book length avg (average length in minutes of all purchases), Price paid_overall (sum of all purchases) ,Price Paid avg (average of all purchases), Review (a Boolean variable whether the customer left a review), Review out of 10 (if the customer left a review, his/her review out of 10, Total minutes listened, Completion (from 0 to 1), Support requests (number of support requests; everything from forgotten password to assistance for using the App), and Last visited minus purchase date (in days).\n",
    "\n",
    "These are the inputs (excluding customer ID, as it is completely arbitrary. It's used only to identify the \"person\", more like a name, than a number).\n",
    "\n",
    "The targets are a Boolean variable (0 or 1). We are taking a period of 2 years in our inputs, and the next 6 months as targets. So, in fact, we are predicting if: based on the last 2 years of activity and engagement, a customer will convert in the next 6 months. 6 months sounds like a reasonable time. If they don't convert after 6 months, chances are they've gone to a competitor or didn't like something about the Audiobook way of dealing information. \n",
    "\n",
    "The task is simple: create a machine learning algorithm, which is able to predict if a customer will buy again.This is a classification problem with two classes: won't buy and will buy, represented by 0s and 1s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data. Balance the dataset. Create 3 datasets: training, validation, and test. Save the newly created sets in a tensor friendly format (.npz)\n",
    "\n",
    "When dealing with real life data,its crucial that we preprocess it to create a good model.\n",
    "\n",
    "If you want to know how to do that, go through the code with comments. In any case, this should do the trick for most datasets organized in the way: many inputs, and then 1 cell containing the targets (supersized learning datasets). Keep in mind that a specific problem may require additional preprocessing.\n",
    "\n",
    "PS: All the header row's(which contain the names of the categories) have been excluded, solely because we just want the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick notes\n",
    "\n",
    "This is a project made by a student of data science, so i made all the comments on the basis of reminding myself of what each thing on the code does, how its useful for data scientists and some explanation on things i should keep in mind while making deep learning models.So keep in mind that a lot of the comments its not useful if you are already an experienced data scientist, but to newcomers(such as myself) can be really useful.\n",
    "\n",
    "Thank you for your attention and have fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries and extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy is a package to work with multidimensional arrays on python, widely used on data science.\n",
    "import numpy as np\n",
    "#Sklearn preprocessing library its used to stardardize the data.\n",
    "from sklearn import preprocessing\n",
    "#This package will only be used in the machine learning algorithm, its not used in the preprocessing of the data.\n",
    "import tensorflow as tf\n",
    "\n",
    "#Load the data and assign it to the variable raw_csv_data\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv',delimiter=',')\n",
    "\n",
    "#The inputs are all columns in the csv, except for the first one [:,0] that is the customer ID(contain no useful information)\n",
    "#and the last one [:,-1], which is our targets\n",
    "\n",
    "#Load all columns except the first(ID's) and the last one(targets) and assign it to unscaled_inpust_all\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1]\n",
    "\n",
    "#The targets are in the last column. That's how datasets are conventionally organized.\n",
    "#Load the targets and assign it to targets_all\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count how many targets are 1(meaning that the customer did convert)\n",
    "num_one_targets = int(np.sum(targets_all))\n",
    "\n",
    "#Set a counter for targets that are 0(meaning that the customer did not convert)\n",
    "zero_targets_counter = 0\n",
    "\n",
    "#We want to create a \"balanced\" dataset, so we will have to remove some input/target pairs.\n",
    "#Declare a variable that will do that:\n",
    "indices_to_remove = []\n",
    "\n",
    "#We want to have the same number of 0s and 1s on targets.\n",
    "#Count the number of targets that are 0. \n",
    "#Once there are as many 0s as 1s, mark entries where the target is 0.\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "#Create two new variables, one that will contain the inputs, and one that will contain the targets.\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "#Delete all the indices that we marked \"to remove\" in the loop above.\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will use the sklearn functionality, which has good preprocessing capabilities.\n",
    "#If you try to run this code without standardizing the inputs, you will get \n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When the data was collected it was actually arranged by date, but to have a good model we have to shuffle that data.\n",
    "#Since we will be batching, we want the data to be as randomly spread out as possible.\n",
    "#This create the variable shuffled_indices and shuffle it randomly, so the data is not arranged in any way when we feed it.\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "#Use the shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791.0 3579 0.5004191114836547\n",
      "226.0 447 0.5055928411633109\n",
      "220.0 448 0.49107142857142855\n"
     ]
    }
   ],
   "source": [
    "#Count the total number of samples and assign it to the samples_count variable.\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "#Always keep in mind that the training dataset must be a lot bigger than the rest.\n",
    "#Count the samples in each subset, assuming we want 80-10-10(commonly used) distribution of training, validation, and test.\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "\n",
    "#We dont need to make the same process as above for the test dataset, just assign it to the all of remaining data.\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "#Create variables that record the inputs and targets for training\n",
    "#In our shuffled dataset, they are the first \"train_samples_count\" observations\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "#Create variables that record the inputs and targets for validation.\n",
    "#They are the next \"validation_samples_count\" observations, folllowing the \"train_samples_count\" we already assigned\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "#Create variables that record the inputs and targets for test.\n",
    "#Again, they are everything that is remaining.\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "#Each time you rerun this code you will get a different value, because they are always shuffled randomly.\n",
    "#Normally you only preprocess once, so you dont need to run the part of the code that preprocess the data everytime.\n",
    "#Check if your training, validation and test are balanced, because they are taken from a shuffled dataset.\n",
    "\n",
    "#Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\n",
    "#You want to make sure that the proportions are close to 50%(0.5), and that the training dataset its alot bigger \n",
    "#than the rest.\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the three datasets in *.npz.\n",
    "#Always remember to name your datasets in a coherent way, in this case, the name of the original file + the dataset.\n",
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code above was to preprocess the data, balance the datasets into training, validation and tests, and save it in the desired format to work in machine learning(.npz).\n",
    "Now it's time to create the machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a temporary variable npz, where we will store each of the three Audiobooks datasets\n",
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "#Extract the inputs using the keyword under which we saved them\n",
    "# to ensure that they are all floats, let's also take care of that\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "# we load the validation data in the temporary variable\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "# we can load the inputs and the targets in the same line\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "#Load the test data in the npz variable.\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "#Create the variables to the tests inputs and targets.\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 0s - loss: 0.5261 - accuracy: 0.8284 - val_loss: 0.3941 - val_accuracy: 0.8837\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.3577 - accuracy: 0.8779 - val_loss: 0.3122 - val_accuracy: 0.8949\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.3152 - accuracy: 0.8838 - val_loss: 0.2912 - val_accuracy: 0.8949\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.2963 - accuracy: 0.8896 - val_loss: 0.2761 - val_accuracy: 0.8993\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.2832 - accuracy: 0.8947 - val_loss: 0.2643 - val_accuracy: 0.9060\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.2752 - accuracy: 0.8963 - val_loss: 0.2545 - val_accuracy: 0.9150\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.2689 - accuracy: 0.8986 - val_loss: 0.2487 - val_accuracy: 0.9150\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.2624 - accuracy: 0.8991 - val_loss: 0.2446 - val_accuracy: 0.9195\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.2579 - accuracy: 0.9011 - val_loss: 0.2399 - val_accuracy: 0.9195\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.2548 - accuracy: 0.9025 - val_loss: 0.2434 - val_accuracy: 0.9195\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.2529 - accuracy: 0.9016 - val_loss: 0.2397 - val_accuracy: 0.9172\n",
      "Epoch 12/100\n",
      "36/36 - 0s - loss: 0.2508 - accuracy: 0.9044 - val_loss: 0.2344 - val_accuracy: 0.9217\n",
      "Epoch 13/100\n",
      "36/36 - 0s - loss: 0.2502 - accuracy: 0.9047 - val_loss: 0.2322 - val_accuracy: 0.9195\n",
      "Epoch 14/100\n",
      "36/36 - 0s - loss: 0.2502 - accuracy: 0.9042 - val_loss: 0.2378 - val_accuracy: 0.9172\n",
      "Epoch 15/100\n",
      "36/36 - 0s - loss: 0.2445 - accuracy: 0.9067 - val_loss: 0.2330 - val_accuracy: 0.9195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d85e385b48>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set the input and output sizes\n",
    "input_size = 10\n",
    "output_size = 2\n",
    "#Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 50\n",
    "    \n",
    "#Define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])\n",
    "\n",
    "\n",
    "#Choose the optimizer and the loss function\n",
    "\n",
    "#We define the optimizer we'd like to use, the loss function, and the metrics we are interested in obtaining at \n",
    "#each iteration.\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Training\n",
    "#That's where we train the model we have built.\n",
    "\n",
    "#Set the batch size\n",
    "batch_size = 100\n",
    "\n",
    "#Set a maximum number of training epochs\n",
    "#This will rarely be the real number of epochs, because in a certain point to continue testing it would only increase the\n",
    "#chances of overfitting the model.\n",
    "max_epochs = 100\n",
    "\n",
    "#Set an early stopping mechanism\n",
    "#We will set the patience=2, to be a bit tolerant against random validation loss increases.\n",
    "#This is very important to stop the training before the model goes into overfitting.\n",
    "#When the validation error starts increasing that might be a indicator of overfitting.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "#Fit the model\n",
    "#Note that this time the train, validation and test data are not iterable\n",
    "model.fit(train_inputs, # train inputs\n",
    "          train_targets, # train targets\n",
    "          batch_size=batch_size, # batch size\n",
    "          epochs=max_epochs, # epochs that we will train for (assuming early stopping doesn't kick in)\n",
    "          # callbacks are functions called by a task when a task is completed\n",
    "          # task here is to check if val_loss is increasing\n",
    "          callbacks=[early_stopping], # early stopping\n",
    "          validation_data=(validation_inputs, validation_targets), # validation data\n",
    "          verbose = 2 # making sure we get enough information about the training process\n",
    "          )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model\n",
    "After training on the training data and validating on the validation data, we test the final prediction power of our model by running it on the test dataset that the algorithm has NEVER seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset.\n",
    "\n",
    "The test is the absolute final instance. You should not test before you are completely done with adjusting your model.\n",
    "\n",
    "If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2342 - accuracy: 0.9196\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.23. Test accuracy: 91.96%\n"
     ]
    }
   ],
   "source": [
    "#Prints the loss and accuracy of the test dataset.\n",
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the initial model and hyperparameters given in this notebook, the final test accuracy should be roughly around 91%.\n",
    "\n",
    "Note that each time the code is rerun, we get a different accuracy because each training is different. \n",
    "\n",
    "Please note this is a suboptimal solution, there's still space to build on it and there's still room to improve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
